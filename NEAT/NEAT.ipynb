{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NEAT to Evolve Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MultiNEAT as NEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was sort of a pain to install, but the good news is it's all set up now and I have openCV 3.3.0 installed as well.  Basically [this](http://multineat.com/docs.html) is the extent of the documentation since I don't believe this library has been widely used.  It's also not fully compatible with the current `boost` version (1.65) because one of the header files was removed (`numeric.hpp`) so I used `cython` to build it instead (although the new files related to Traits require me to have `boost` installed anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NEAT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEAT stands for [\"NeuroEvolution of Augmenting Topologies\"](https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies) and is a genetic algorithm for evolving neural networks.  It is based on the idea that mutation, speciation, and natural selection can begin with very simple structures and eventually evolve complex topologies that maximize a given fitness function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are really two distinct ways to use NEAT:\n",
    "\n",
    "    1. Decide on the topology of the neural network ahead of time and simply evolve connection\n",
    "    weights and biases.\n",
    "\n",
    "![caption](files/PreDetermTopo.png)\n",
    "\n",
    "    2. Start with a very simple network consisting of only input and output layers and evolve new nodes\n",
    "    and connections along with connection weights and biases.\n",
    "\n",
    "![caption](files/EvolvedTopo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolving a Neural Network That Outputs [1,0]  (A Tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with NEAT, I am going to go through a toy example.  It will involve initializing a population of simple genomes, constructing neural network phenotypes based on these genomes, defining an evaluation function to determine the fitness of a given phenotype, and defining an epoch cycle to iterate through new generations. (NOTE: This code will primarily come from the [documentation page](http://multineat.com/docs.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = NEAT.Parameters()\n",
    "\n",
    "params.PopulationSize = 100\n",
    "\n",
    "# The Genome constructor (in C++) is copied below\n",
    "# Genome::Genome(unsigned int a_ID,\n",
    "#                    unsigned int a_NumInputs,\n",
    "#                    unsigned int a_NumHidden, // ignored for seed type == 0, specifies number of hidden units if seed type == 1\n",
    "#                    unsigned int a_NumOutputs,\n",
    "#                    bool a_FS_NEAT, ActivationFunction a_OutputActType,\n",
    "#                    ActivationFunction a_HiddenActType,\n",
    "#                    unsigned int a_SeedType,\n",
    "#                    const Parameters &a_Parameters)\n",
    "\n",
    "genome = NEAT.Genome(0, 3, 0, 2, False,\n",
    "                     NEAT.ActivationFunction.UNSIGNED_SIGMOID,\n",
    "                     NEAT.ActivationFunction.UNSIGNED_SIGMOID, 0, params)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author of the documentation (likely also the author of the library) tells us to: \"Always add one extra input. The last input is always used as bias and also when you activate the network always set the last input to 1.0 (or any other constant non-zero value).\" Knowing this, the network we defined just now has 2 'real' inputs and 2 outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Population constructor (again in C++) is copied below\n",
    "# Population::Population(const Genome& a_Seed, const Parameters& a_Parameters,\n",
    "#     bool a_RandomizeWeights, double a_RandomizationRange, int a_RNG_seed)\n",
    "pop = NEAT.Population(genome, params, True, 1.0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a population of 100 simple neural network genomes with randomized weights.  Next step is to define an evaluation function, which will require us to build phenotypes from these genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(genome):\n",
    "\n",
    "    # this creates a neural network (phenotype) from the genome\n",
    "    net = NEAT.NeuralNetwork()\n",
    "    genome.BuildPhenotype(net)\n",
    "\n",
    "    # let's input just one pattern to the net, activate it once and get the output\n",
    "    net.Input( [ 1.0, 0.0, 1.0 ] )\n",
    "    net.Activate()\n",
    "    output = net.Output() \n",
    "\n",
    "    # the output can be used as any other Python iterable.\n",
    "    # instead of the original decision to select for '0' output on the first output,\n",
    "    # I will select for the output [1,0]\n",
    "    fitness = output[0] + (1 - output[1])\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest fitness in generation 0: 1.3847487012811497\n",
      "Percentage of Max Fitness: 0.6923743506405748\n",
      "Highest fitness in generation 1: 1.5667443711378604\n",
      "Percentage of Max Fitness: 0.7833721855689302\n",
      "Highest fitness in generation 2: 1.7236779176066068\n",
      "Percentage of Max Fitness: 0.8618389588033034\n",
      "Highest fitness in generation 3: 1.7819764915947478\n",
      "Percentage of Max Fitness: 0.8909882457973739\n",
      "Highest fitness in generation 4: 1.8672971334927315\n",
      "Percentage of Max Fitness: 0.9336485667463658\n",
      "Highest fitness in generation 5: 1.9253129945621064\n",
      "Percentage of Max Fitness: 0.9626564972810532\n",
      "Highest fitness in generation 6: 1.9544168711332208\n",
      "Percentage of Max Fitness: 0.9772084355666104\n",
      "Highest fitness in generation 7: 1.9651953333932402\n",
      "Percentage of Max Fitness: 0.9825976666966201\n",
      "Highest fitness in generation 8: 1.9723779739344118\n",
      "Percentage of Max Fitness: 0.9861889869672059\n",
      "Highest fitness in generation 9: 1.976634900085418\n",
      "Percentage of Max Fitness: 0.988317450042709\n"
     ]
    }
   ],
   "source": [
    "for generation in range(10): # run for 10 generations\n",
    "\n",
    "    # retrieve a list of all genomes in the population\n",
    "    genome_list = NEAT.GetGenomeList(pop)\n",
    "\n",
    "    fitness_list = []\n",
    "    # apply the evaluation function to all genomes\n",
    "    for genome in genome_list:\n",
    "        fitness = evaluate(genome)\n",
    "        fitness_list.append(fitness)\n",
    "        genome.SetFitness(fitness)\n",
    "\n",
    "    # Here is where the author recommends to output information on the current generation\n",
    "    top_fit = sorted(fitness_list)[-1]\n",
    "    print(\"Highest fitness in generation \" + str(generation) + \": \" + str(top_fit))\n",
    "    print(\"Percentage of Max Fitness: \" + str(top_fit/2.0))\n",
    "\n",
    "    # advance to the next generation\n",
    "    pop.Epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This at least provides good evidence that the library is working as intended, although this problem was so simple that by generation 9 (the 10th generation) the most fit neural network has 99% of maximum fitness (perfect outputs).  However, the massive difference between this task and the training of neural networks in previous notebooks is that these neural networks acted on a single input and tried to match the expected output.  This is like getting a single training example 99% right out of the 60,000 in MNIST and then reporting 99% accuracy.\n",
    "\n",
    "One could imagine instead providing the neural networks with all the inputs from the UCI dataset or even MNIST, and then seeing how well evolution can develop an accurate regression.  This will be a replacement of backpropagation, with the mechanism for reducing error being the 'natural' selection of weights which produce higher fitness.  This is what I will attempt to do now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution vs. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "\n",
    "FILE_NAME = \"drug_consumption_data.txt\"\n",
    "NUM_FEATURES = 12\n",
    "\n",
    "with open(FILE_NAME) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "features = []\n",
    "usages = []\n",
    "cases = []\n",
    "for line in lines:\n",
    "    # First elem in line is an id number\n",
    "    line = line.strip('\\n').split(',')[1:]\n",
    "    feature = line[:NUM_FEATURES]\n",
    "    usage = line[NUM_FEATURES:]\n",
    "    features.append(feature)\n",
    "    usages.append(usage)\n",
    "    cases.append((feature,usage))\n",
    "    \n",
    "def classify_usage(usage_code):\n",
    "    if usage_code in constants.USER:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def process_case(case):\n",
    "    x, y = case\n",
    "    return (list(map(float, x)), list(map(classify_usage, y)))\n",
    "cases = list(map(process_case, cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is taken directly from my [VisualizingData notebook](https://emdoyle.github.io/tensorflow_stuff/VisualizingData.html) since I like the format of `cases`.  However there is some very annoying work to be done to this data.  Since many of the input features are categorical (gender, country, ethnicity etc.) these need to be encoded into one-hot groups of input nodes.  This means that for example gender information will be distributed across two nodes.  When the case indicates that the participant is Male, the first node will be 1 and the second node will be 0.  This is reversed for Female.\n",
    "\n",
    "This was done automatically in a couple of the previous notebooks when using TensorFlow's `tf.feature_column.bucketized_column`.  In fact I am just realizing now why those columns were Sparse columns as opposed to real-valued Dense columns.  Anyways this process is important to the accuracy of our classifier (mostly for examples with more than two options however) because we do not want it to think there is a closer relationship between categories which are coded into numbers which are close together versus numbers which are far apart.  Hopefully this makes sense but if not there is a good explanation [here](https://www.researchgate.net/post/How_to_code_categorical_inputs_for_a_neural_network) which is very short and to the point.\n",
    "\n",
    "I am going to house the pre-processing code which turns our real-valued info into sparse categorical input in [this file](https://github.com/emdoyle/tensorflow_stuff/tree/master/NEAT/utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "cases = utils.expand_features(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first entry of cases now, we can see that there is a mix of real values and what appear to be embedded one-hot vectors.  This is the format of our features input now:\n",
    "\n",
    "000000 | 00 | real | 0000000 | 0000000 | real | real | real | real | real | real | real\n",
    "\n",
    "where the 0s represent a one-hot vector with the same length.\n",
    "\n",
    "This leaves us with a new number of input nodes:\n",
    "\n",
    "6 + 2 + 1 + 7 + 7 + 7(1) = 30 input nodes\n",
    "\n",
    "We will come back to this information later, but until then we can rest easy that our data is processed exactly as we'd like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Just verifying the math\n",
    "print(len(cases[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = NEAT.Parameters()\n",
    "\n",
    "params.PopulationSize = 150\n",
    "\n",
    "params.MinSpecies = 5\n",
    "params.MaxSpecies = 10\n",
    "params.RouletteWheelSelection = False\n",
    "params.RecurrentProb = 0.0\n",
    "params.OverallMutationRate = 0.8\n",
    "\n",
    "params.MutateWeightsProb = 0.90\n",
    "\n",
    "params.WeightMutationMaxPower = 2.5\n",
    "params.WeightReplacementMaxPower = 5.0\n",
    "params.MutateWeightsSevereProb = 0.9\n",
    "params.WeightMutationRate = 0.25\n",
    "\n",
    "params.MaxWeight = 8\n",
    "\n",
    "params.MutateAddNeuronProb = 0.03\n",
    "params.MutateAddLinkProb = 0.05\n",
    "params.MutateRemLinkProb = 0.0\n",
    "\n",
    "params.ActivationFunction_SignedSigmoid_Prob = 1.0\n",
    "params.ActivationFunction_UnsignedSigmoid_Prob = 0.0\n",
    "params.ActivationFunction_Tanh_Prob = 0.0\n",
    "params.ActivationFunction_SignedStep_Prob = 0.0\n",
    "\n",
    "params.CrossoverRate = 0.75  # mutate only 0.25\n",
    "params.MultipointCrossoverRate = 0.4\n",
    "params.SurvivalRate = 0.2\n",
    "\n",
    "params.AllowLoops = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above was copied from [this example file](https://github.com/peter-ch/MultiNEAT/blob/master/examples/TestNEAT_xor.py) because it quickly shows me a ton of the possible configurable parameters.  I'll leave them as are for now.\n",
    "\n",
    "Now onto writing the evaluation function.  The task of this function is to take in a genome, build the corresponding NeuralNetwork phenotype, run through inputs and outputs, calculate error, and return the fitness based on the observed error.  The choice of how to calculate the error is up to me, so this is another place I may modify to get better results later.\n",
    "\n",
    "Unfortunately, I will be using a built-in function that expects `evaluate` to have this exact function signature, so I can't add another `target` parameter which would allow for easy selection of a target.  Instead I'll use a global variable since it is more visible than changing a variable in `constants.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = utils.DRUG_INDEXES[\"cannabis\"]\n",
    "\n",
    "def calc_error(output, usages):\n",
    "    error = 0\n",
    "    usages = utils.one_hot(2, usages)\n",
    "    usage = usages[TARGET]\n",
    "    \n",
    "    for i in range(2):\n",
    "        error += pow(usage[i] - output[i], 2)\n",
    "    error /= 2\n",
    "    \n",
    "    return error\n",
    "\n",
    "def evaluate(genome):\n",
    "    net = NEAT.NeuralNetwork()\n",
    "    genome.BuildPhenotype(net)\n",
    "\n",
    "    error = 0\n",
    "    \n",
    "    for features, usages in cases:\n",
    "        net.Flush()\n",
    "        net.Input(features)\n",
    "        net.Activate()\n",
    "        o = net.Output()\n",
    "        error += calc_error(o, usages)\n",
    "    \n",
    "    return 1 - (error / len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1, Best fitness: 0.7637449188499649\n",
      "Generation: 10, Best fitness: 0.7669842512733134\n",
      "Generation: 20, Best fitness: 0.7698355346697721\n",
      "Generation: 30, Best fitness: 0.7700872097170129\n",
      "Generation: 40, Best fitness: 0.7700872097170129\n",
      "Generation: 50, Best fitness: 0.7700872097170129\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from MultiNEAT import EvaluateGenomeList_Serial\n",
    "from MultiNEAT import GetGenomeList, ZipFitness\n",
    "\n",
    "g = NEAT.Genome(0, 31, 0, 2, False, NEAT.ActivationFunction.UNSIGNED_SIGMOID,\n",
    "                NEAT.ActivationFunction.UNSIGNED_SIGMOID, 0, params)\n",
    "pop = NEAT.Population(g, params, True, 1.0, 0)\n",
    "pop.RNG.Seed(int(time.clock()*100))\n",
    "\n",
    "generations = 0\n",
    "for generation in range(50):\n",
    "    genome_list = NEAT.GetGenomeList(pop)\n",
    "    fitness_list = EvaluateGenomeList_Serial(genome_list, evaluate, display=False)\n",
    "    NEAT.ZipFitness(genome_list, fitness_list)\n",
    "    pop.Epoch()\n",
    "    generations = generation\n",
    "    best = max(fitness_list)\n",
    "    if generation == 0 or (generation+1) % 10 == 0:\n",
    "        print(\"Generation: \" + str(generation+1) + \", Best fitness: \" + str(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although 50 Generations is actually not that many for this task, I have tried numerous different configurations and achieved what I would call poor results so far.  I think that the problem may be that the fitness is based on minimizing mean squared error across the samples.  The networks may evolve better if the process is more results-oriented (in the spirit of evolution).  To achieve this I will instead calculate the overall accuracy (using argmax of the two output neurons to compare to the one-hot usage vectors) as fitness for each genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_output(output, usages):\n",
    "    usages = utils.one_hot(2, usages)\n",
    "    usage = usages[TARGET]\n",
    "    \n",
    "    if output[0] > output[1]:\n",
    "        if usage[0] > usage[1]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if usage[1] > usage[0]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def evaluate(genome):\n",
    "    net = NEAT.NeuralNetwork()\n",
    "    genome.BuildPhenotype(net)\n",
    "\n",
    "    total_correct = 0\n",
    "    \n",
    "    for features, usages in cases:\n",
    "        net.Flush()\n",
    "        net.Input(features)\n",
    "        net.Activate()\n",
    "        o = net.Output()\n",
    "        total_correct += check_output(o, usages)\n",
    "    \n",
    "    return float(total_correct/len(cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1, Best fitness: 0.4716180371352785\n",
      "Generation: 10, Best fitness: 0.4880636604774536\n",
      "Generation: 20, Best fitness: 0.4880636604774536\n",
      "Generation: 30, Best fitness: 0.4880636604774536\n",
      "Generation: 40, Best fitness: 0.4880636604774536\n",
      "Generation: 50, Best fitness: 0.4880636604774536\n",
      "Generation: 60, Best fitness: 0.4880636604774536\n",
      "Generation: 70, Best fitness: 0.4880636604774536\n",
      "Generation: 80, Best fitness: 0.4880636604774536\n",
      "Generation: 90, Best fitness: 0.48859416445623344\n",
      "Generation: 100, Best fitness: 0.48859416445623344\n",
      "Generation: 110, Best fitness: 0.48859416445623344\n",
      "Generation: 120, Best fitness: 0.48859416445623344\n",
      "Generation: 130, Best fitness: 0.48859416445623344\n",
      "Generation: 140, Best fitness: 0.48859416445623344\n",
      "Generation: 150, Best fitness: 0.48859416445623344\n",
      "Generation: 160, Best fitness: 0.48859416445623344\n",
      "Generation: 170, Best fitness: 0.48859416445623344\n",
      "Generation: 180, Best fitness: 0.48859416445623344\n",
      "Generation: 190, Best fitness: 0.48859416445623344\n",
      "Generation: 200, Best fitness: 0.48859416445623344\n",
      "Generation: 210, Best fitness: 0.48859416445623344\n",
      "Generation: 220, Best fitness: 0.48859416445623344\n",
      "Generation: 230, Best fitness: 0.48859416445623344\n",
      "Generation: 240, Best fitness: 0.48859416445623344\n",
      "Generation: 250, Best fitness: 0.48859416445623344\n",
      "Generation: 260, Best fitness: 0.48859416445623344\n",
      "Generation: 270, Best fitness: 0.48859416445623344\n",
      "Generation: 280, Best fitness: 0.48859416445623344\n",
      "Generation: 290, Best fitness: 0.48859416445623344\n",
      "Generation: 300, Best fitness: 0.48859416445623344\n",
      "Generation: 310, Best fitness: 0.48859416445623344\n",
      "Generation: 320, Best fitness: 0.48859416445623344\n",
      "Generation: 330, Best fitness: 0.48859416445623344\n",
      "Generation: 340, Best fitness: 0.4891246684350133\n",
      "Generation: 350, Best fitness: 0.4891246684350133\n",
      "Generation: 360, Best fitness: 0.4891246684350133\n",
      "Generation: 370, Best fitness: 0.4891246684350133\n",
      "Generation: 380, Best fitness: 0.4891246684350133\n",
      "Generation: 390, Best fitness: 0.4891246684350133\n",
      "Generation: 400, Best fitness: 0.4891246684350133\n",
      "Generation: 410, Best fitness: 0.4891246684350133\n",
      "Generation: 420, Best fitness: 0.4891246684350133\n",
      "Generation: 430, Best fitness: 0.4891246684350133\n",
      "Generation: 440, Best fitness: 0.4891246684350133\n",
      "Generation: 450, Best fitness: 0.4891246684350133\n",
      "Generation: 460, Best fitness: 0.4891246684350133\n",
      "Generation: 470, Best fitness: 0.4891246684350133\n",
      "Generation: 480, Best fitness: 0.4891246684350133\n",
      "Generation: 490, Best fitness: 0.4891246684350133\n",
      "Generation: 500, Best fitness: 0.4891246684350133\n"
     ]
    }
   ],
   "source": [
    "g = NEAT.Genome(0, 31, 10, 2, False, NEAT.ActivationFunction.SIGNED_SIGMOID,\n",
    "                NEAT.ActivationFunction.SIGNED_SIGMOID, 0, params)\n",
    "pop = NEAT.Population(g, params, True, 1.0, 0)\n",
    "pop.RNG.Seed(int(time.clock()*100))\n",
    "\n",
    "generations = 0\n",
    "for generation in range(500):\n",
    "    genome_list = NEAT.GetGenomeList(pop)\n",
    "    fitness_list = EvaluateGenomeList_Serial(genome_list, evaluate, display=False)\n",
    "    NEAT.ZipFitness(genome_list, fitness_list)\n",
    "    pop.Epoch()\n",
    "    generations = generation\n",
    "    best = max(fitness_list)\n",
    "    if generation == 0 or (generation+1) % 10 == 0:\n",
    "        print(\"Generation: \" + str(generation+1) + \", Best fitness: \" + str(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty terrible.  I can't tell if the Genome just starts out as such a terrible classifier that random mutations will take too long to find a good solution or if I have implemented some part of the process incorrectly.\n",
    "\n",
    "Actually, after looking at the source code for `Parameters`, I found what may be the problem: stagnation!  As the process is implemented, after 50 generations without an improvement in a species, the species dies off.  This is a problem if I don't expect improvements at such a quick pace.  I will re-run the same experiment with a stagnation period of 500 generations (effectively disabling stagnation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 1, Best fitness: 0.4716180371352785\n",
      "Generation: 10, Best fitness: 0.4870026525198939\n",
      "Generation: 20, Best fitness: 0.4870026525198939\n",
      "Generation: 30, Best fitness: 0.4870026525198939\n",
      "Generation: 40, Best fitness: 0.4870026525198939\n",
      "Generation: 50, Best fitness: 0.4870026525198939\n",
      "Generation: 60, Best fitness: 0.4870026525198939\n",
      "Generation: 70, Best fitness: 0.4870026525198939\n",
      "Generation: 80, Best fitness: 0.4870026525198939\n",
      "Generation: 90, Best fitness: 0.4870026525198939\n",
      "Generation: 100, Best fitness: 0.4870026525198939\n",
      "Generation: 110, Best fitness: 0.4870026525198939\n",
      "Generation: 120, Best fitness: 0.4870026525198939\n",
      "Generation: 130, Best fitness: 0.48753315649867374\n",
      "Generation: 140, Best fitness: 0.48753315649867374\n",
      "Generation: 150, Best fitness: 0.48753315649867374\n",
      "Generation: 160, Best fitness: 0.48753315649867374\n",
      "Generation: 170, Best fitness: 0.48753315649867374\n",
      "Generation: 180, Best fitness: 0.48753315649867374\n",
      "Generation: 190, Best fitness: 0.48753315649867374\n",
      "Generation: 200, Best fitness: 0.48753315649867374\n",
      "Generation: 210, Best fitness: 0.4880636604774536\n",
      "Generation: 220, Best fitness: 0.4880636604774536\n",
      "Generation: 230, Best fitness: 0.4880636604774536\n",
      "Generation: 240, Best fitness: 0.4880636604774536\n",
      "Generation: 250, Best fitness: 0.4880636604774536\n",
      "Generation: 260, Best fitness: 0.4880636604774536\n",
      "Generation: 270, Best fitness: 0.4880636604774536\n",
      "Generation: 280, Best fitness: 0.4880636604774536\n",
      "Generation: 290, Best fitness: 0.4880636604774536\n",
      "Generation: 300, Best fitness: 0.4880636604774536\n",
      "Generation: 310, Best fitness: 0.4880636604774536\n",
      "Generation: 320, Best fitness: 0.4880636604774536\n",
      "Generation: 330, Best fitness: 0.4880636604774536\n",
      "Generation: 340, Best fitness: 0.4880636604774536\n",
      "Generation: 350, Best fitness: 0.4880636604774536\n",
      "Generation: 360, Best fitness: 0.4880636604774536\n",
      "Generation: 370, Best fitness: 0.4880636604774536\n",
      "Generation: 380, Best fitness: 0.4880636604774536\n",
      "Generation: 390, Best fitness: 0.4880636604774536\n",
      "Generation: 400, Best fitness: 0.4880636604774536\n",
      "Generation: 410, Best fitness: 0.4880636604774536\n",
      "Generation: 420, Best fitness: 0.4880636604774536\n",
      "Generation: 430, Best fitness: 0.4880636604774536\n",
      "Generation: 440, Best fitness: 0.4880636604774536\n",
      "Generation: 450, Best fitness: 0.4880636604774536\n",
      "Generation: 460, Best fitness: 0.4880636604774536\n",
      "Generation: 470, Best fitness: 0.4880636604774536\n",
      "Generation: 480, Best fitness: 0.4880636604774536\n",
      "Generation: 490, Best fitness: 0.4880636604774536\n",
      "Generation: 500, Best fitness: 0.4880636604774536\n"
     ]
    }
   ],
   "source": [
    "params.SpeciesMaxStagnation = 500\n",
    "\n",
    "g = NEAT.Genome(0, 31, 10, 2, False, NEAT.ActivationFunction.SIGNED_SIGMOID,\n",
    "                NEAT.ActivationFunction.SIGNED_SIGMOID, 0, params)\n",
    "pop = NEAT.Population(g, params, True, 1.0, 0)\n",
    "pop.RNG.Seed(int(time.clock()*100))\n",
    "\n",
    "generations = 0\n",
    "for generation in range(500):\n",
    "    genome_list = NEAT.GetGenomeList(pop)\n",
    "    fitness_list = EvaluateGenomeList_Serial(genome_list, evaluate, display=False)\n",
    "    NEAT.ZipFitness(genome_list, fitness_list)\n",
    "    pop.Epoch()\n",
    "    generations = generation\n",
    "    best = max(fitness_list)\n",
    "    if generation == 0 or (generation+1) % 10 == 0:\n",
    "        print(\"Generation: \" + str(generation+1) + \", Best fitness: \" + str(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did _NOT_ work.  I think this is a pretty strong signal that this library is not going to be a good alternative to backpropagation for me.  The only way I can think of to give it another chance is to figure out how to load in an already-trained Genome so that NEAT can focus on evolving the topology of the network and ignore the weights.  Actually I lied, I have one other idea that I saw on [HN](https://news.ycombinator.com) a week or so ago, which is to use NEAT solely to evolve hyperparameters (like learning rate, activation function, number of hidden nodes) as a tuple.  I don't think this library is flexible enough to do something like that, but it would be relatively easy to put it together myself.  However that kind of process would be very time-consuming to run and would likely bump accuracy a few points at the most (it is a polishing step).  These ideas both represent a continued investment in this algorithm which I am not interested in making.  It was cool to see it work on the tiny toy case and I saw first-hand that it does not beat gradient-descent given a similar timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
