{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow to Predict Drug Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will be building an Estimator using tf.estimator to classify drug usage in the [UCI dataset](https://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29) and comparing my results to the accompanying [paper](https://arxiv.org/abs/1506.06297).  The paper will also give me some insight into how to customize the Estimator to each classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import constants\n",
    "\n",
    "from utils import compare_results, process_column_names, one_hot\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "# Less Verbose Output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "DATA_DIR = \"Dataset/\"\n",
    "DRUG_TRAINING = DATA_DIR + \"re_drug_consumption_data.csv\"\n",
    "DRUG_TEST = DATA_DIR + \"test_drug_consumption_data.csv\"\n",
    "DRUG_PREDICT = DATA_DIR + \"predict_drug_consumption_data.csv\"\n",
    "\n",
    "PREDICT_OUTPUT = DATA_DIR + \"predictions.csv\"\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Bucketization for possible nonlinear relationship\n",
    "age = tf.feature_column.numeric_column(\"age\")\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=constants.AGE_BOUNDARIES)\n",
    "gender = tf.feature_column.numeric_column(\"gender\")\n",
    "gender_buckets = tf.feature_column.bucketized_column(\n",
    "    gender, boundaries=constants.GENDER_BOUNDARIES)\n",
    "country = tf.feature_column.numeric_column(\"country\")\n",
    "country_buckets = tf.feature_column.bucketized_column(\n",
    "    country, boundaries=constants.COUNTRY_BOUNDARIES)\n",
    "ethnicity = tf.feature_column.numeric_column(\"ethnicity\")\n",
    "ethnicity_buckets = tf.feature_column.bucketized_column(\n",
    "    ethnicity, boundaries=constants.ETHNICITY_BOUNDARIES)\n",
    "education = tf.feature_column.numeric_column(\"education\")\n",
    "education_buckets = tf.feature_column.bucketized_column(\n",
    "    education, boundaries=constants.EDUCATION_BOUNDARIES)\n",
    "\n",
    "# Could bucketize but guessing these are close to linear\n",
    "nscore = tf.feature_column.numeric_column(\"nscore\")\n",
    "escore = tf.feature_column.numeric_column(\"escore\")\n",
    "oscore = tf.feature_column.numeric_column(\"oscore\")\n",
    "ascore = tf.feature_column.numeric_column(\"ascore\")\n",
    "cscore = tf.feature_column.numeric_column(\"cscore\")\n",
    "impulsive = tf.feature_column.numeric_column(\"impulsive\")\n",
    "ss = tf.feature_column.numeric_column(\"ss\")\n",
    "\n",
    "nscore_ascore = tf.feature_column.crossed_column(\n",
    "    [\"nscore\", \"ascore\"], hash_bucket_size=500)\n",
    "nscore_cscore = tf.feature_column.crossed_column(\n",
    "    [\"nscore\", \"cscore\"], hash_bucket_size=500)\n",
    "ascore_cscore = tf.feature_column.crossed_column(\n",
    "    [\"ascore\", \"cscore\"], hash_bucket_size=500)\n",
    "nscore_ascore_cscore = tf.feature_column.crossed_column(\n",
    "    [\"nscore\", \"ascore\", \"cscore\"], hash_bucket_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_file, target, num_epochs, feature_columns, batch_size=30, shuffle=False, num_threads=1):\n",
    "    dataset = pd.read_csv(\n",
    "        tf.gfile.Open(data_file),\n",
    "        header=0,\n",
    "        usecols=feature_columns + [target],\n",
    "        skipinitialspace=True,\n",
    "        engine=\"python\")\n",
    "    # Drop NaN entries\n",
    "    dataset.dropna(how=\"any\", axis=0)\n",
    "\n",
    "    # Init empty dataframe, add column for each of targets\n",
    "    labels = pd.DataFrame(columns=[target])\n",
    "    \n",
    "    # This assigns a different number to each usage category\n",
    "    # labels[constants.TARGET] = dataset[constants.TARGET].apply(lambda x: constants.MAPPED_CODES[x]).astype(int)\n",
    "\n",
    "    # This classifies usage as binary (USER/NON-USER) to make prediction easier\n",
    "    labels[target] = dataset[target].apply(lambda x: x in constants.USER).astype(int)\n",
    "    dataset.pop(target)\n",
    "    \n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": np.array(dataset)},\n",
    "        y=np.array(one_hot(NUM_CLASSES, labels[target])),\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        num_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is a hacky solution due to the following issue:\n",
    "# the tf.layers weights are called dense_[number]/kernel:0\n",
    "# but while tf.GraphKeys.TRAINABLE_VARIABLES records dense/kernel:0, dense_1/kernel:0...\n",
    "# layer.name for some reason records dense/kernel:0, dense_2/kernel:0...\n",
    "# So I need to decrement the number on the end of 'dense' if there is one.\n",
    "def extract_weights(layer):\n",
    "    name = os.path.split(layer.name)[0]\n",
    "    if '_' in name:\n",
    "        number = str(int(name[name.find('_')+1:]) - 1)\n",
    "        name = name[:name.find('_')] + '_' + number\n",
    "    for variable in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "        if variable.name == name + '/kernel:0':\n",
    "            return variable\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \n",
    "    # 1. Configure the model via TensorFlow operations\n",
    "    input_layer = tf.cast(features[\"x\"], tf.float32)\n",
    "    \n",
    "    beta = params[\"beta\"]\n",
    "    layer_sizes = params[\"hidden_layers\"]\n",
    "    current_tensor = input_layer\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(beta)\n",
    "    weights = []\n",
    "    for nodes in layer_sizes:\n",
    "        current_tensor = tf.layers.dense(current_tensor, nodes, activation=tf.nn.tanh,\n",
    "            kernel_regularizer=regularizer)\n",
    "        weights.append(extract_weights(current_tensor))\n",
    "        \n",
    "    output_layer = tf.layers.dense(current_tensor, 2, activation=tf.nn.tanh)\n",
    "\n",
    "    # 4. Generate predictions\n",
    "    predictions = output_layer\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions={\"usage\": predictions})\n",
    "    \n",
    "    # 2. Define the loss function for training/evaluation\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=labels))\n",
    "    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "    loss += reg_term\n",
    "    loss = tf.divide(loss, tf.constant(len(params[\"hidden_layers\"]), tf.float32))\n",
    "\n",
    "    eval_metric_ops = {\n",
    "        \"rmse\": tf.metrics.root_mean_squared_error(tf.cast(labels,tf.float64), tf.cast(predictions,tf.float64)),\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            tf.cast(tf.argmax(labels,1), tf.float64), tf.cast(tf.argmax(predictions,1), tf.float64))\n",
    "    }\n",
    "    \n",
    "    # 3. Define the training operation/optimizer\n",
    "    decay_steps = 100000\n",
    "    learning_rate = tf.train.polynomial_decay(params[\"start_learn\"], tf.train.get_global_step(),\n",
    "                                          decay_steps, params[\"end_learn\"],\n",
    "                                          power=0.5)\n",
    "    optimizer=tf.train.GradientDescentOptimizer(\n",
    "        learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # 5. Return predictions/loss/train_op/eval_metric_ops in EstimatorSpec object\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(classifier, target, feature_cols, batch_size=25, steps=100000):\n",
    "    classifier.train(input_fn=input_fn(DRUG_TRAINING, feature_columns=feature_cols, target=target,\n",
    "        batch_size=batch_size, num_epochs=None, shuffle=True), steps=steps)\n",
    "\n",
    "    results = classifier.evaluate(input_fn=input_fn(DRUG_TEST, target=target, num_epochs=1,\n",
    "        feature_columns=feature_cols, shuffle=False), steps=None)\n",
    "\n",
    "    print(results)\n",
    "    print(\"Accuracy: %.2f%%\" % (results['accuracy']*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(classifier, target, feature_cols, cutoff=0):\n",
    "    predictions = classifier.predict(input_fn=input_fn(DRUG_PREDICT, target=target, num_epochs=1,\n",
    "        feature_columns=feature_cols, shuffle=False))\n",
    "    predict_writer = open(PREDICT_OUTPUT, \"w\")\n",
    "    predict_writer.write(\"Fake header\\n\")\n",
    "    for prediction in list(predictions):\n",
    "        val1 = prediction[\"usage\"][0]\n",
    "        val2 = prediction[\"usage\"][1]\n",
    "        if val1 > val2:\n",
    "            predict_writer.write('0\\n')\n",
    "        else:\n",
    "            predict_writer.write('1\\n')\n",
    "    predict_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.81333333, 'loss': 0.22922964, 'rmse': 0.84780484, 'global_step': 100000}\n",
      "Accuracy: 81.33%\n",
      "Comparing 200:200\n",
      "cannabis Accuracy: 75.000000%\n",
      "cannabis Sensitivity: 74.545455%\n",
      "cannabis Specificity: 75.555556%\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "    age_buckets, education_buckets,\n",
    "    oscore, ascore, cscore, impulsive, ss\n",
    "]\n",
    "\n",
    "feature_col_names = process_column_names(feature_columns)\n",
    "\n",
    "model_params = {\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"hidden_layers\": [10, 5],\n",
    "    \"start_learn\": 0.1,\n",
    "    \"end_learn\": 0.01,\n",
    "    \"beta\": 0.001\n",
    "}\n",
    "\n",
    "nn = tf.estimator.Estimator(model_fn, params=model_params)\n",
    "train_and_eval(nn, \"cannabis\", feature_col_names)\n",
    "custom_predict(nn, \"cannabis\", feature_col_names)\n",
    "compare_results([\"cannabis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimator runs 100,000 training steps pretty quickly (about two minutes on my laptop), and has achieved better accuracy than we have seen so far on this dataset.  A few choices that I made with this model are:\n",
    "\n",
    "1. Used polynomial decay to decay the learning rate.\n",
    "  * This slows the learning rate as the model sees more examples to avoid overshooting a local loss minimum\n",
    "2. Used a GradientDescentOptimizer.\n",
    "  * This is the only optimizer which I have examined in any detail mathematically, I would like to experiment with others though. Changing from the ProximalAdagradOptimizer was a matter of necessity however.\n",
    "3. Used mean squared error for my loss function.\n",
    "  * I'm not sure what loss function the pre-built estimators use but this choice was made similarly to the optimizer\n",
    "4. Shrunk the network to two hidden layers of 10 and 5 nodes respectively\n",
    "  * It occurred to me that with only a 12 node input layer and a single node output layer, it makes very little intuitive sense to have hidden layers larger than 12.  This way, the network can be thought of as a funnel, concentrating the information present in the inputs into ultimately a single node.  This is not very mathematical but it did produce the best results in the least amount of time so I will definitely explore this idea further.\n",
    "5. Used l2 regularization to avoid overfitting\n",
    "  * The sensitivity and specificity on prediction weren't looking great so I added l2 regularization to the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using this new model to predict other types of drug usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.82666665, 'loss': 0.23683901, 'rmse': 0.94360751, 'global_step': 100000}\n",
      "Accuracy: 82.67%\n",
      "Comparing 200:200\n",
      "alcohol Accuracy: 83.000000%\n",
      "alcohol Sensitivity: 83.589744%\n",
      "alcohol Specificity: 60.000000%\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "    age_buckets, gender_buckets, education_buckets,\n",
    "    nscore, ss\n",
    "]\n",
    "\n",
    "feature_col_names = process_column_names(feature_columns)\n",
    "\n",
    "model_params = {\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"hidden_layers\": [10, 5],\n",
    "    \"start_learn\": 0.15,\n",
    "    \"end_learn\": 0.01,\n",
    "    \"beta\": 0.001\n",
    "}\n",
    "\n",
    "nn = tf.estimator.Estimator(model_fn, params=model_params)\n",
    "train_and_eval(nn, \"alcohol\", feature_col_names)\n",
    "custom_predict(nn, \"alcohol\", feature_col_names)\n",
    "compare_results([\"alcohol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.62666667, 'loss': 0.22827935, 'rmse': 0.89752734, 'global_step': 60000}\n",
      "Accuracy: 62.67%\n",
      "Comparing 200:200\n",
      "nicotine Accuracy: 56.000000%\n",
      "nicotine Sensitivity: 54.444444%\n",
      "nicotine Specificity: 57.272727%\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "    gender_buckets,\n",
    "    nscore, escore, cscore\n",
    "]\n",
    "\n",
    "feature_col_names = process_column_names(feature_columns)\n",
    "\n",
    "model_params = {\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"hidden_layers\": [25,10,5],\n",
    "    \"start_learn\": 0.1,\n",
    "    \"end_learn\": 0.01,\n",
    "    \"beta\": 0.001\n",
    "}\n",
    "\n",
    "nn = tf.estimator.Estimator(model_fn, params=model_params)\n",
    "train_and_eval(nn, \"nicotine\", feature_col_names, steps=60000)\n",
    "custom_predict(nn, \"nicotine\", feature_col_names)\n",
    "compare_results([\"nicotine\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a bit surprising to me.  I tried many different combinations of hidden layers, feature columns, learning rate, and even the number of total training steps to see if I could get an accuracy above 70% and did not succeed.  The authors of the [paper](https://arxiv.org/abs/1506.06297) accompanying this dataset claim to have achieved around 70% sensitivity and specifity for each drug, so I think I will read over their findings and see if I can apply some of them to my own model.  Although if the authors were only able to achieve 70% on classifying nicotine usage, then I'm not too far off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9066667, 'loss': 0.14956614, 'rmse': 0.85296059, 'global_step': 100000}\n",
      "Accuracy: 90.67%\n",
      "Comparing 200:200\n",
      "LSD Accuracy: 87.500000%\n",
      "LSD Sensitivity: 46.153846%\n",
      "LSD Specificity: 90.374332%\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "    age_buckets, gender_buckets,\n",
    "    nscore, escore, oscore, impulsive\n",
    "]\n",
    "\n",
    "feature_col_names = process_column_names(feature_columns)\n",
    "\n",
    "model_params = {\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"hidden_layers\": [10,5],\n",
    "    \"start_learn\": 0.1,\n",
    "    \"end_learn\": 0.01,\n",
    "    \"beta\": 0.001\n",
    "}\n",
    "\n",
    "nn = tf.estimator.Estimator(model_fn, params=model_params)\n",
    "train_and_eval(nn, \"LSD\", feature_col_names)\n",
    "custom_predict(nn, \"LSD\", feature_col_names)\n",
    "compare_results([\"LSD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first an accuracy over 90% looked awesome, but I was curious to see the sensitivity in particular.  Since LSD usage as defined occurs in such a small minority of the respondents, the model adopts a heavy negative bias.  It is very bad at identifying people who actually use LSD, and its accuracy is buoyed by the fact that this failure doesn't come up very often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning Selection Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading Table 18 on page 34 of [the study accompanying the dataset](https://arxiv.org/pdf/1506.06297.pdf)\n",
    "I decided to be much more selective about which features to use.  They recommend at most 6 features and at\n",
    "fewest 2 features should be used for a given classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.82666665, 'loss': 0.51349771, 'rmse': 0.93510514, 'global_step': 100000}\n",
      "Accuracy: 82.67%\n",
      "Comparing 200:200\n",
      "alcohol Accuracy: 82.000000%\n",
      "alcohol Sensitivity: 82.741117%\n",
      "alcohol Specificity: 33.333333%\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [\n",
    "    age_buckets, education_buckets, gender_buckets,\n",
    "    nscore, ss\n",
    "]\n",
    "\n",
    "feature_col_names = process_column_names(feature_columns)\n",
    "\n",
    "model_params = {\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"hidden_layers\": [10],\n",
    "    \"start_learn\": 0.15,\n",
    "    \"end_learn\": 0.01,\n",
    "    \"beta\": 0.001\n",
    "}\n",
    "\n",
    "nn = tf.estimator.Estimator(model_fn, params=model_params)\n",
    "train_and_eval(nn, \"alcohol\", feature_col_names)\n",
    "custom_predict(nn, \"alcohol\", feature_col_names)\n",
    "compare_results([\"alcohol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
