{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Deep Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will be re-implementing a deep neural net that I put together in a previous notebook, this time using the TensorFlow low-level API to gain familiarity and perhaps achieve better performance (both speed and accuracy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using [this](https://www.tensorflow.org/get_started/mnist/pros) to help me get started.  I also used a script found [here](https://gist.githubusercontent.com/ischlag/41d15424e7989b936c1609b53edd1390/raw/5ed7aca47bcca30b3df1c3bfd0f027e6bcdb430c/mnist-to-jpg.py) to pre-process the MNIST data to more closely resemble something I could put together from a random dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "# Tensors for input layer and output layer\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, IMAGE_PIXELS])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "\n",
    "dataset_path      = \"mnist/\"\n",
    "test_labels_file  = \"test-labels.csv\"\n",
    "train_labels_file = \"train-labels.csv\"\n",
    "\n",
    "def read_label_file(file):\n",
    "    f = open(file, \"r\")\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    for line in f:\n",
    "        filepath, label = line.split(\",\")\n",
    "        filepaths.append(filepath)\n",
    "        labels.append(int(label))\n",
    "    return filepaths, labels\n",
    "\n",
    "test_filepaths, test_labels = read_label_file(dataset_path + test_labels_file)\n",
    "train_filepaths, train_labels = read_label_file(dataset_path + train_labels_file)\n",
    "\n",
    "TRAIN_CASES = len(train_filepaths)\n",
    "TEST_CASES = len(test_filepaths)\n",
    "\n",
    "def input_parser(img_path, label):\n",
    "    # convert the label to one-hot encoding\n",
    "    one_hot = tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "    # read the img from file\n",
    "    img_file = tf.read_file(img_path)\n",
    "    img_decoded = tf.image.decode_image(img_file)\n",
    "    img_decoded = tf.reshape(tf.cast(img_decoded, tf.float32), [IMAGE_PIXELS])\n",
    "    img_decoded = tf.div(img_decoded,tf.constant(255.0))\n",
    "\n",
    "    return img_decoded, one_hot\n",
    "\n",
    "# Create Dataset objects\n",
    "train_data = Dataset.from_tensor_slices((train_filepaths, train_labels))\n",
    "test_data = Dataset.from_tensor_slices((test_filepaths, test_labels))\n",
    "\n",
    "train_data = train_data.map(input_parser, num_threads=8, output_buffer_size=100*BATCH_SIZE)\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "test_data = test_data.map(input_parser, num_threads=8, output_buffer_size=100*BATCH_SIZE)\n",
    "test_data = test_data.batch(TEST_CASES)\n",
    "\n",
    "# Create Iterator objects\n",
    "iterator = Iterator.from_structure(train_data.output_types,\n",
    "                                   train_data.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# Create two initialization ops to switch between the datasets\n",
    "training_init_op = iterator.make_initializer(train_data)\n",
    "testing_init_op = iterator.make_initializer(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I have two ops, each of which will allow me to iterate over the Datasets I created.  One corresponds to training data and the other to testing/evaluation data.  Below I print the shape of the first element of each to see that the processing has worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape:0\", shape=(2,), dtype=int32), Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"Shape_2:0\", shape=(2,), dtype=int32), Tensor(\"Shape_3:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(training_init_op)\n",
    "\n",
    "    try:\n",
    "        elem = sess.run(next_element)\n",
    "        print(str(tf.shape(elem[0])) + \", \" + str(tf.shape(elem[1])))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of training dataset.\")\n",
    "\n",
    "    sess.run(testing_init_op)\n",
    "\n",
    "    try:\n",
    "        elem = sess.run(next_element)\n",
    "        print(str(tf.shape(elem[0])) + \", \" + str(tf.shape(elem[1])))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of test dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element in each tuple is a batch of arrays of IMAGE_PIXELS integers.  The second element in each tuple is a batch of arrays of NUM_CLASSES integers and is one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulled from TensorBoard tutorial\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 250]\n",
      "[250, 30]\n",
      "[30, 10]\n",
      "[10, 10]\n",
      "[1, 250]\n",
      "[1, 30]\n",
      "[1, 10]\n",
      "[1, 10]\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYERS = [250,30,10]\n",
    "WEIGHTS = [0 for z in range(0,len(HIDDEN_LAYERS)+1)]\n",
    "BIASES = [0 for z in range(0, len(HIDDEN_LAYERS)+1)]\n",
    "\n",
    "# These are from one of the tutorials, helpful utility functions\n",
    "def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, dtype='float32')\n",
    "\n",
    "def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, dtype='float32')\n",
    "\n",
    "# This is adapted from my non-TF implementation of init_weights()\n",
    "with tf.name_scope(\"weights\"):\n",
    "    for i in range(0, len(WEIGHTS)):\n",
    "        if i == 0:\n",
    "            WEIGHTS[i] = weight_variable([IMAGE_PIXELS,HIDDEN_LAYERS[i]])\n",
    "        if i == len(WEIGHTS)-1:\n",
    "            # Add one to first dim to account for bias on hidden layers\n",
    "            WEIGHTS[i] = weight_variable([HIDDEN_LAYERS[-1],NUM_CLASSES])\n",
    "        if i != 0 and i != len(WEIGHTS)-1:\n",
    "            WEIGHTS[i] = weight_variable([HIDDEN_LAYERS[i-1],HIDDEN_LAYERS[i]])\n",
    "    \n",
    "        variable_summaries(WEIGHTS[i])\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    for i in range(0, len(BIASES)-1):\n",
    "        BIASES[i] = bias_variable([1,HIDDEN_LAYERS[i]])\n",
    "        variable_summaries(BIASES[i])\n",
    "    \n",
    "    BIASES[-1] = bias_variable([1,NUM_CLASSES])\n",
    "    variable_summaries(BIASES[-1])\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    for weight in WEIGHTS:\n",
    "        print(weight.get_shape().as_list())\n",
    "        \n",
    "    for bias in BIASES:\n",
    "        print(bias.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now I can see that the weights and biases are being shaped correctly relative to the hidden layers, all that is left is to implement the backpropagation algorithm.\n",
    "\n",
    "[This](http://blog.aloni.org/posts/backprop-with-tensorflow/) has turned out to be extremely helpful in manually modeling backpropagation in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_HIDDENS = [0 for z in WEIGHTS]\n",
    "ACT_HIDDENS = [0 for z in WEIGHTS]\n",
    "\n",
    "# First is a special case, rest are general\n",
    "with tf.name_scope(\"hidden_layers\"):\n",
    "    NET_HIDDENS[0] = tf.add(tf.matmul(x,WEIGHTS[0]),BIASES[0])\n",
    "    ACT_HIDDENS[0] = tf.sigmoid(NET_HIDDENS[0])\n",
    "\n",
    "    for i in range(1,len(NET_HIDDENS)):\n",
    "        NET_HIDDENS[i] = tf.add(tf.matmul(ACT_HIDDENS[i-1],WEIGHTS[i]),BIASES[i])\n",
    "        ACT_HIDDENS[i] = tf.sigmoid(NET_HIDDENS[i])\n",
    "\n",
    "# Last activated layer is the network's guess, y_ is the answer\n",
    "with tf.name_scope(\"cost\"):\n",
    "    cost = tf.divide(tf.subtract(ACT_HIDDENS[-1],y_),tf.constant(NUM_CLASSES, tf.float32))\n",
    "with tf.name_scope(\"xentropy\"):\n",
    "    xentropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=ACT_HIDDENS[-1]))\n",
    "    tf.summary.scalar('xentropy', xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is essentially the forward portion of the algorithm.  The backpropagation step is unnecessary in TF, since it apparently can 'derive the step function.'  I will look into this further but for now I will assume that it means backpropagation is a particular type of step function or something like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is taken from the link mentioned above, not available in tf activation functions\n",
    "def sigmoidprime(x):\n",
    "    return tf.multiply(tf.sigmoid(x), tf.subtract(tf.constant(1.0), tf.sigmoid(x)))\n",
    "\n",
    "# Number of WEIGHTS and BIASES should be the same\n",
    "DELTA_ZS = [0 for z in WEIGHTS]\n",
    "DELTA_AS = [0 for z in WEIGHTS]\n",
    "DELTA_WEIGHTS = [0 for z in WEIGHTS]\n",
    "DELTA_BIASES = [0 for z in BIASES]\n",
    "\n",
    "DELTA_AS[-1] = cost\n",
    "\n",
    "# First case is again the exception to the rule\n",
    "with tf.name_scope(\"deltas\"):\n",
    "    DELTA_ZS[-1] = tf.multiply(DELTA_AS[-1],sigmoidprime(NET_HIDDENS[-1]))\n",
    "    DELTA_BIASES[-1] = DELTA_ZS[-1]\n",
    "    DELTA_WEIGHTS[-1] = tf.matmul(tf.transpose(ACT_HIDDENS[-2]),DELTA_ZS[-1])\n",
    "\n",
    "    for i in range(len(DELTA_ZS)-2,0,-1):\n",
    "        DELTA_AS[i] = tf.matmul(DELTA_ZS[i+1],tf.transpose(WEIGHTS[i+1]))\n",
    "        DELTA_ZS[i] = tf.multiply(DELTA_AS[i],sigmoidprime(NET_HIDDENS[i]))\n",
    "        DELTA_BIASES[i] = DELTA_ZS[i]\n",
    "        DELTA_WEIGHTS[i] = tf.matmul(tf.transpose(ACT_HIDDENS[i-1]),DELTA_ZS[i])\n",
    "    \n",
    "    # Last case is also an exception\n",
    "    DELTA_AS[0] = tf.matmul(DELTA_ZS[1],tf.transpose(WEIGHTS[1]))\n",
    "    DELTA_ZS[0] = tf.multiply(DELTA_AS[0],sigmoidprime(NET_HIDDENS[0]))\n",
    "    DELTA_BIASES[0] = DELTA_ZS[0]\n",
    "    DELTA_WEIGHTS[0] = tf.matmul(tf.transpose(x),DELTA_ZS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the tutorial\n",
    "eta = tf.constant(float(1/(len(HIDDEN_LAYERS)+1)))\n",
    "step = [0 for z in range(0,len(WEIGHTS)*2)]\n",
    "\n",
    "with tf.name_scope(\"global_step\"):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1.0\n",
    "end_learning_rate = 0.01\n",
    "decay_steps = TRAIN_CASES/2\n",
    "with tf.name_scope(\"learning_rate\"):\n",
    "    learning_rate = tf.train.polynomial_decay(starter_learning_rate, global_step,\n",
    "                                          decay_steps, end_learning_rate,\n",
    "                                          power=0.5)\n",
    "    tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "with tf.name_scope(\"step\"):\n",
    "    for i in range(0,len(WEIGHTS)):\n",
    "        step[2*i] = tf.assign(WEIGHTS[i], tf.subtract(WEIGHTS[i],tf.multiply(learning_rate,DELTA_WEIGHTS[i])))\n",
    "        step[(2*i)+1] = tf.assign(BIASES[i], tf.subtract(BIASES[i],\n",
    "                                                     tf.multiply(learning_rate,tf.reduce_mean(DELTA_BIASES[i], axis=[0]))))\n",
    "incr_global_step = tf.assign(global_step, tf.add(tf.constant(1, tf.int32), global_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCHES = int(TRAIN_CASES/BATCH_SIZE)\n",
    "\n",
    "loss_datapoints = []\n",
    "\n",
    "acct_mat = tf.equal(tf.argmax(ACT_HIDDENS[-1], 1), tf.argmax(y_, 1))\n",
    "acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "GRAPH_DIR = '/tmp/mnist_model'\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(GRAPH_DIR + \"/run\" + str(RUN), sess.graph)\n",
    "    sess.run(training_init_op)\n",
    "\n",
    "    try:\n",
    "        for p in range(BATCHES):\n",
    "            batch_xs, batch_ys = sess.run(next_element)\n",
    "            summary, _ = sess.run([merged,step], feed_dict = {x : batch_xs,\n",
    "                                y_ : batch_ys})\n",
    "            if p % 1000 == 0:\n",
    "                summary_writer.add_summary(summary, p + i*BATCHES)\n",
    "            sess.run(incr_global_step)\n",
    "            loss_snapshot = sess.run(xentropy, feed_dict = {x : batch_xs,\n",
    "                                y_ : batch_ys})\n",
    "            loss_datapoints.append(loss_snapshot)\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"End of training dataset.\")\n",
    "\n",
    "    if i == (EPOCHS-1):\n",
    "        sess.run(testing_init_op)\n",
    "\n",
    "        try:\n",
    "            all_xs, all_ys = sess.run(next_element)\n",
    "            result = sess.run(acct_res, feed_dict = {x : all_xs,\n",
    "                                    y_ : all_ys})\n",
    "            print(\"Accuracy: %.2f%%\" % ((result/len(all_ys))*100))\n",
    "    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of test dataset.\")\n",
    "\n",
    "RUN += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!  This is a deep neural network capable of a variable number of hidden nodes and layers written using the TensorFlow low-level API.  Next I will add bells and whistles until it can beat 98.40% (the best I achieved without TF).  \n",
    "Then, I think it'd be a good idea to try to package it as an Estimator (although I'm not sure this is possible or even makes sense since I haven't looked into it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 6\n",
    "\n",
    "plt.xlabel('Batches',fontsize=16)\n",
    "plt.ylabel('Error', fontsize=16)\n",
    "plt.title('Average Loss')\n",
    "plt.grid(True)\n",
    "plt.plot(range(BATCHES*EPOCHS), loss_datapoints, linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't really what I'm looking for, I just added up the 'cost' for each of the classes for each batch.  I'm going to experiment with other 'cost' formulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
